# Discrete Prototypical Memories for Federated Time Series Foundation Models

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/release/python-390/)
[![PyTorch 2.3](https://img.shields.io/badge/PyTorch-2.3-ee4c2c.svg)](https://pytorch.org/get-started/locally/)

Official PyTorch implementation for **FedUnit**, a **Fed**erated framework for learning **Uni**fied **T**ime-series representations via discrete prototypical memories.

## ğŸ¯ Overview

<p align="center">
<img src="./Images/Overview.png" width="900">
<br>
<em>Figure 1. The overall architecture of the proposed FedUnit framework.</em>
</p>

## ğŸ“‚ Project Structure

```text
.
â”œâ”€â”€ data_provider/          # Data loading and factory
â”œâ”€â”€ exp/                   # Core experiment execution logic
â”œâ”€â”€ layers/                # Custom neural network layers (Transformer, etc.)
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ models/            # VQ-VAE and backbone model implementations
â”‚   â””â”€â”€ utils/             # Loggers, checkpoints, and metrics
â”œâ”€â”€ federated_learning_main.py  # Main entry point for training
â””â”€â”€ federated_components.py      # Federated server and client definitions
```

## ğŸ“ Install Dependencies

First, ensure you have the required packages installed.

```bash
pip install -r requirements.txt
```


## ğŸ‘‰ Data Preparation

### 1. Download Datasets
Please [download here](https://drive.google.com/drive/folders/1vE0ONyqPlym2JaaAoEe0XNDR8FS_d322) and place the `.csv` files into the `Datasets` folder:

```text
Datasets/
â””â”€â”€ imputation_and_forecasting_data/
    â”œâ”€â”€ electricity/
    â”‚   â””â”€â”€ electricity.csv 
    â”œâ”€â”€ weather/
    â”‚   â””â”€â”€ Weather.csv
    â”œâ”€â”€ exchange_rate/
    â”‚   â””â”€â”€ exchange_rate.csv
    â””â”€â”€ ETT-small/
        â”œâ”€â”€ ETTh1.csv
        â”œâ”€â”€ ETTh2.csv
        â”œâ”€â”€ ETTm1.csv
        â””â”€â”€ ETTm2.csv
```

### 2. Preprocessing
Run the following script to generate `.npy` cache Files in `saved_data/`:
```bash
bash extract_all_data.sh
```

## ğŸš€ Experiments

### 1.Full-Shot Federated Learning
Train the model across all clients using the default federated setting:
```bash
bash train_FL_setting.sh
```

### 2.Few-Shot Learning (5% & 10%)
We evaluate the framework's robustness using only a fraction of the local data:
*   **5% Data**: `bash few_shot_5percent.sh`
*   **10% Data**: `bash few_shot_10percent.sh`

## âš™ï¸ Configuration
You can customize the training by modifying:
*   `vqvae.json`: Codebook size, embedding dimension, and compression factor.
*   `Ablation_args_transformer.yaml`: Backbone Transformer hyperparameters.
*   `cuda_id`: GPU device ID in the `.sh` scripts.

## ğŸŒŸ Citation

If you find this work is helpful to your research, please consider citing:

```
Coming Soon!
```
Thanks for your interest in our work!
